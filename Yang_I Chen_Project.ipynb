{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>DSCI-552 Project</h1></center>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I Chen Yang\n",
    "### GitHub ID:ichenyang\n",
    "### 8260207588"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras version 2.8.0\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import seaborn as sns; sns.set(color_codes=True)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(f\"Keras version {keras.__version__}\")\n",
    "print(tf.config.list_physical_devices() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### i. You can use binary encoding for the sentiments , i.e y = 1 for positive sentiments and y = âˆ’1 for negative sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "Paths = []\n",
    "X = [] # X are input string data\n",
    "Y = [] # y\\in{1,-1} are labels\n",
    "\n",
    "for data_path in ['../data/neg', '../data/pos']:\n",
    "  for filename in os.listdir(data_path):\n",
    "    f = os.path.join(data_path, filename)\n",
    "    if os.path.isfile(f) and filename.endswith(\".txt\"):\n",
    "      with open(f, \"r\") as text_file:\n",
    "        read_text = text_file.read()\n",
    "        Paths.append(f)\n",
    "        X.append(read_text)\n",
    "        Y.append(2*(data_path == 'pos')-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ii. The data are pretty clean. Remove the punctuation and numbers from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "\n",
    "X_clean = []\n",
    "for x in X:\n",
    "  for ele in x:\n",
    "    if ele in punc:\n",
    "        x = x.replace(ele, \"\")\n",
    "  X_clean.append(''.join(i for i in x if not i.isdigit()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### iii. The name of each text file starts with cv number. Use text files 0-699 in each class for training and 700-999 for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "x_test = []\n",
    "y_train = []\n",
    "y_test = []\n",
    "for p, x, y in zip(Paths, X_clean, Y):\n",
    "  p_id = int(p.split('cv')[-1].split('_')[0])\n",
    "  if(p_id <= 699):\n",
    "      x_train.append(x)\n",
    "      y_train.append(y)\n",
    "  else:\n",
    "      x_test.append(x)\n",
    "      y_test.append(y)\n",
    "\n",
    "assert len(x_train)==1400 and len(y_train)==1400 and len(x_test)==600 and len(y_test)==600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### iv. Count the number of unique words in the whole dataset (train + test) and print it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 47156\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "for text in x_train + x_test:\n",
    "  t = text.lower()\n",
    "  word = t.split()\n",
    "  words = words + word\n",
    "\n",
    "print(f\"Number of unique words: {len(set(words))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### v. Calculate the average review length and the standard deviation of review lengths. Report the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average review length: 644.5415 words\n",
      "S.D. of review length: 285.01493518366715 words\n"
     ]
    }
   ],
   "source": [
    "num_words = []\n",
    "for text in x_train + x_test:\n",
    "  t = text.lower()\n",
    "  word = t.split()\n",
    "  num_words.append(len(word))\n",
    "\n",
    "print(f\"Average review length: {np.mean(num_words)} words\")\n",
    "print(f\"S.D. of review length: {np.std(num_words)} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### vi. Plot the histogram of review lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 0, 'Review length in words')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gfg = sns.histplot(data=num_words)\n",
    "gfg.set(xlabel =\"Review length in words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### vii. To represent each text (= data point), there are many ways. In NLP/Deep Learning terminology, this task is called tokenization. It is common to represent text using popularity/ rank of words in text. The most common word in the text will be represented as 1, the second most common word will be represented as 2, etc. Tokenize each text document using this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1073\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(x_train) # To Check: should x_test be included?\n",
    "\n",
    "# Encode training data sentences into sequences\n",
    "train_sequences = t.texts_to_sequences(x_train)\n",
    "test_sequences = t.texts_to_sequences(x_test)\n",
    "print(len(train_sequences[0]))\n",
    "\n",
    "# # Get max training sequence length\n",
    "# maxlen = max([len(x) for x in train_sequences])\n",
    "# print(maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Dropout, Activation, Embedding, Flatten\n",
    "\n",
    "# integer encode the documents\n",
    "vocab_size = 5000\n",
    "train_sequences = [one_hot(d, vocab_size) for d in x_train]\n",
    "test_sequences = [one_hot(d, vocab_size) for d in x_test]\n",
    "print(len(encoded_docs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### viii. Select a review length L that 70% of the reviews have a length below it. If you feel more adventurous, set the threshold to 90%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ans: One can determine L by looking-up the inverse CDF of review length at ~70%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L = 737\n"
     ]
    }
   ],
   "source": [
    "x, counts = np.unique(num_words, return_counts=True)\n",
    "cusum = np.cumsum(counts)\n",
    "cdf = cusum / cusum[-1]\n",
    "L = x[np.where(cdf > 0.7)][0]\n",
    "print(f\"L = {L}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ix. Truncate reviews longer than L words and zero-pad reviews shorter than L so that all texts (= data points) are of length L."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Truncate/Pad the training/testing sequences\n",
    "train_padded = pad_sequences(train_sequences, padding='post', truncating='post', maxlen=L)\n",
    "test_padded  = pad_sequences(test_sequences, padding='post', truncating='post', maxlen=L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_padded = train_padded*(train_padded <= 4999)\n",
    "test_padded = test_padded*(test_padded <= 4999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1400, 737, 32)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding\n",
    "emb_layer = Embedding(5000, 32, input_length=L)\n",
    "x_emb = emb_layer(train_padded)\n",
    "print(x_emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1400, 23584)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Flatten\n",
    "flat_layer = Flatten()\n",
    "x_emb_flat = flat_layer(x_emb)\n",
    "print(x_emb_flat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[45, 1], [10, 10], [8, 34], [11, 10], [18], [44], [31, 34], [29, 10], [31, 10], [12, 47, 1, 37]]\n",
      "[[45  1  0  0]\n",
      " [10 10  0  0]\n",
      " [ 8 34  0  0]\n",
      " [11 10  0  0]\n",
      " [18  0  0  0]\n",
      " [44  0  0  0]\n",
      " [31 34  0  0]\n",
      " [29 10  0  0]\n",
      " [31 10  0  0]\n",
      " [12 47  1 37]]\n",
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_7 (Embedding)     (None, 4, 8)              400       \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 433\n",
      "Trainable params: 433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "1/1 [==============================] - 0s 424ms/step - loss: 0.6996 - accuracy: 0.3000\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6983 - accuracy: 0.4000\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6970 - accuracy: 0.4000\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6957 - accuracy: 0.4000\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6944 - accuracy: 0.5000\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5000\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6918 - accuracy: 0.5000\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6906 - accuracy: 0.6000\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6893 - accuracy: 0.7000\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6881 - accuracy: 0.7000\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6868 - accuracy: 0.7000\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6856 - accuracy: 0.7000\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6843 - accuracy: 0.7000\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6831 - accuracy: 0.7000\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6818 - accuracy: 0.7000\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6806 - accuracy: 0.7000\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6793 - accuracy: 0.7000\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6781 - accuracy: 0.7000\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6768 - accuracy: 0.7000\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6756 - accuracy: 0.7000\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6743 - accuracy: 0.7000\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6730 - accuracy: 0.7000\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6717 - accuracy: 0.7000\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6704 - accuracy: 0.7000\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6691 - accuracy: 0.8000\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6678 - accuracy: 0.8000\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6665 - accuracy: 0.8000\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6652 - accuracy: 0.8000\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6638 - accuracy: 0.8000\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6625 - accuracy: 0.8000\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6611 - accuracy: 0.8000\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6597 - accuracy: 0.8000\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6583 - accuracy: 0.8000\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6569 - accuracy: 0.9000\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6555 - accuracy: 0.9000\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6541 - accuracy: 0.9000\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6527 - accuracy: 0.9000\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6512 - accuracy: 0.9000\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6497 - accuracy: 0.9000\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6483 - accuracy: 0.9000\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6468 - accuracy: 0.9000\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6453 - accuracy: 0.9000\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6438 - accuracy: 0.9000\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.6422 - accuracy: 0.9000\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6407 - accuracy: 0.9000\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6391 - accuracy: 0.9000\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6376 - accuracy: 0.9000\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6360 - accuracy: 0.9000\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6344 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6328 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.6312 - accuracy: 1.0000\n",
      "Accuracy: 100.000000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Dropout, Activation, Embedding, Flatten\n",
    "# test\n",
    "docs = ['Well done!',\n",
    "\t\t'Good work',\n",
    "\t\t'Great effort',\n",
    "\t\t'nice work',\n",
    "\t\t'Excellent!',\n",
    "\t\t'Weak',\n",
    "\t\t'Poor effort!',\n",
    "\t\t'not good',\n",
    "\t\t'poor work',\n",
    "\t\t'Could have done better.']\n",
    "\n",
    "# define class labels\n",
    "labels = [1,1,1,1,1,0,0,0,0,0,]\n",
    "# integer encode the documents\n",
    "vocab_size = 50\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
    "print(encoded_docs)\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "# fit the model\n",
    "model.fit(padded_docs, np.array(labels), epochs=50)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, np.array(labels))\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 737, 32)           160000    \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         multiple                  0         \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 50)                1179250   \n",
      "                                                                 \n",
      " dropout_35 (Dropout)        (None, 50)                0         \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " dropout_36 (Dropout)        (None, 50)                0         \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        (None, 50)                0         \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,344,401\n",
      "Trainable params: 1,344,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(emb_layer)\n",
    "model.add(flat_layer)\n",
    "\n",
    "model.add(tf.keras.layers.Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) ii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "140/140 [==============================] - 1s 6ms/step - loss: -609842.8125 - accuracy: 0.0000e+00\n",
      "Epoch 2/2\n",
      "140/140 [==============================] - 1s 6ms/step - loss: -40400012.0000 - accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:5 out of the last 42 calls to <function Model.make_test_function.<locals>.test_function at 0x7feeb6f7aca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 210ms/step - loss: -124930728.0000 - accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:6 out of the last 43 calls to <function Model.make_test_function.<locals>.test_function at 0x7feeb6f7aca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 131ms/step - loss: -124617936.0000 - accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(train_padded, np.array(y_train)*1, batch_size=10, epochs=2)\n",
    "\n",
    "# get train accuracy\n",
    "loss, trn_accuracy = model.evaluate(train_padded, np.array(y_train), batch_size=len(train_padded))\n",
    "\n",
    "# evaluate the model\n",
    "loss, tst_accuracy = model.evaluate(test_padded, np.array(y_test), batch_size=len(test_padded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy = 0.00 %\n",
      "test  accuracy = 0.00 %\n"
     ]
    }
   ],
   "source": [
    "print(f\"train accuracy = {trn_accuracy*100:.2f} %\")\n",
    "print(f\"test  accuracy = {tst_accuracy*100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e) i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 737, 32)           160000    \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 737, 32)           3104      \n",
      "                                                                 \n",
      " max_pooling1d_3 (MaxPooling  (None, 368, 32)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         multiple                  0         \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 50)                588850    \n",
      "                                                                 \n",
      " dropout_27 (Dropout)        (None, 50)                0         \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " dropout_28 (Dropout)        (None, 50)                0         \n",
      "                                                                 \n",
      " dense_40 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " dropout_29 (Dropout)        (None, 50)                0         \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 757,105\n",
      "Trainable params: 757,105\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Dropout, Activation, Conv1D, MaxPooling1D\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(emb_layer)\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(flat_layer)\n",
    "model.add(tf.keras.layers.Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e) ii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "140/140 [==============================] - 2s 9ms/step - loss: -3039061.0000 - accuracy: 0.0000e+00\n",
      "Epoch 2/2\n",
      "140/140 [==============================] - 1s 9ms/step - loss: -540534272.0000 - accuracy: 0.0000e+00\n",
      "19/19 [==============================] - 0s 4ms/step - loss: -2095281792.0000 - accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_padded, np.array(y_train)*1, batch_size=10, epochs=2)\n",
    "\n",
    "# evaluate the model(Test)\n",
    "loss, accuracy = model.evaluate(test_padded, np.array(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - 0s 3ms/step - loss: -2099345792.0000 - accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model(Train)\n",
    "loss, accuracy = model.evaluate(train_padded, np.array(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (f) i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 737, 32)           160000    \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 100)               53200     \n",
      "                                                                 \n",
      " dropout_30 (Dropout)        (None, 100)               0         \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 256)               25856     \n",
      "                                                                 \n",
      " dropout_31 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 239,313\n",
      "Trainable params: 239,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Dropout, Activation, LSTM\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(emb_layer)\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (f) ii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "140/140 [==============================] - 26s 177ms/step - loss: -189.3428 - accuracy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "140/140 [==============================] - 36s 257ms/step - loss: -1365.5400 - accuracy: 0.0000e+00\n",
      "Epoch 3/20\n",
      "140/140 [==============================] - 41s 296ms/step - loss: -3783.3086 - accuracy: 0.0000e+00\n",
      "Epoch 4/20\n",
      "140/140 [==============================] - 46s 330ms/step - loss: -7416.2900 - accuracy: 0.0000e+00\n",
      "Epoch 5/20\n",
      "140/140 [==============================] - 49s 353ms/step - loss: -12153.0225 - accuracy: 0.0000e+00\n",
      "Epoch 6/20\n",
      "140/140 [==============================] - 45s 317ms/step - loss: -18120.0449 - accuracy: 0.0000e+00\n",
      "Epoch 7/20\n",
      "140/140 [==============================] - 32s 228ms/step - loss: -25083.9062 - accuracy: 0.0000e+00\n",
      "Epoch 8/20\n",
      "140/140 [==============================] - 30s 212ms/step - loss: -33247.3320 - accuracy: 0.0000e+00\n",
      "Epoch 9/20\n",
      "140/140 [==============================] - 29s 209ms/step - loss: -42405.5859 - accuracy: 0.0000e+00\n",
      "Epoch 10/20\n",
      "140/140 [==============================] - 30s 212ms/step - loss: -52436.2227 - accuracy: 0.0000e+00\n",
      "Epoch 11/20\n",
      "140/140 [==============================] - 32s 229ms/step - loss: -63347.6328 - accuracy: 0.0000e+00\n",
      "Epoch 12/20\n",
      "140/140 [==============================] - 40s 283ms/step - loss: -75162.7031 - accuracy: 0.0000e+00\n",
      "Epoch 13/20\n",
      "140/140 [==============================] - 40s 282ms/step - loss: -87864.6562 - accuracy: 0.0000e+00\n",
      "Epoch 14/20\n",
      "140/140 [==============================] - 38s 274ms/step - loss: -101477.8594 - accuracy: 0.0000e+00\n",
      "Epoch 15/20\n",
      "140/140 [==============================] - 38s 268ms/step - loss: -115471.1797 - accuracy: 0.0000e+00\n",
      "Epoch 16/20\n",
      "140/140 [==============================] - 38s 274ms/step - loss: -130956.9453 - accuracy: 0.0000e+00\n",
      "Epoch 17/20\n",
      "140/140 [==============================] - 39s 275ms/step - loss: -146509.9844 - accuracy: 0.0000e+00\n",
      "Epoch 18/20\n",
      "140/140 [==============================] - 38s 272ms/step - loss: -163441.8125 - accuracy: 0.0000e+00\n",
      "Epoch 19/20\n",
      "140/140 [==============================] - 38s 269ms/step - loss: -180456.7969 - accuracy: 0.0000e+00\n",
      "Epoch 20/20\n",
      "140/140 [==============================] - 35s 247ms/step - loss: -198203.8906 - accuracy: 0.0000e+00\n",
      "19/19 [==============================] - 4s 182ms/step - loss: -207735.0781 - accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_padded, np.array(y_train)*1, batch_size=10, epochs=20)\n",
    "\n",
    "# evaluate the model(test)\n",
    "loss, accuracy = model.evaluate(test_padded, np.array(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 3s 160ms/step - loss: -207735.0781 - accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model(train)\n",
    "loss, accuracy = model.evaluate(test_padded, np.array(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
